#!/usr/bin/env python3
"""
SEEK - Optimized Bulk Import Script
==================================

Ultra-fast PostgreSQL COPY FROM import for Texas county data.
Expected performance: 50,000+ records/second vs 4 records/second with REST API.

Usage:
    python optimized_bulk_import.py data/CleanedCsv/tx_bexar_filtered_clean.csv
"""

import os
import sys
import pandas as pd
import psycopg2
import time
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional
from dotenv import load_dotenv
from io import StringIO
import csv

# Load environment variables
load_dotenv()

class OptimizedBulkImporter:
    """Ultra-fast PostgreSQL COPY FROM importer."""
    
    def __init__(self, csv_file_path: str):
        self.csv_file_path = Path(csv_file_path)
        self.conn = self._create_postgres_connection()
        
        # Optimized configuration
        self.batch_size = 50000  # Much larger batches for COPY FROM
        
        # Statistics tracking
        self.stats = {
            'start_time': datetime.now(),
            'records_processed': 0,
            'total_records': 0,
            'batches_processed': 0,
            'counties_created': 0,
            'cities_created': 0,
            'parcels_created': 0,
            'errors': 0
        }
        
        # Cache for performance
        self.texas_state_id = None
        self.county_id = None
        self.cities_cache = {}
        
    def _create_postgres_connection(self):
        """Create direct PostgreSQL connection for maximum performance."""
        # Get database connection details
        supabase_url = os.environ.get('SUPABASE_URL')
        db_password = os.environ.get('SUPABASE_DB_PASSWORD')
        
        if not supabase_url:
            print("‚ùå SUPABASE_URL not found in environment variables")
            sys.exit(1)
        
        if not db_password:
            print("‚ùå SUPABASE_DB_PASSWORD not found in environment variables")
            sys.exit(1)
            
        # Extract project ID from Supabase URL
        import re
        match = re.search(r'https://([^.]+)\.supabase\.co', supabase_url)
        if not match:
            print("‚ùå Could not parse project ID from SUPABASE_URL")
            sys.exit(1)
            
        project_id = match.group(1)
        host = f"db.{project_id}.supabase.co"
        
        print(f"üîó Connecting to PostgreSQL: {host}")
            
        try:
            conn = psycopg2.connect(
                host=host,
                database="postgres",
                user="postgres", 
                password=db_password,
                port=5432,
                connect_timeout=30
            )
            conn.autocommit = False  # We'll control transactions
            print(f"‚úÖ Direct PostgreSQL connection established")
            return conn
            
        except Exception as e:
            print(f"‚ùå Failed to connect to PostgreSQL: {e}")
            print(f"Host: {host}")
            print(f"Database: postgres")
            print(f"User: postgres")
            print("Please verify your SUPABASE_DB_PASSWORD is correct")
            sys.exit(1)
    
    def _optimize_postgres_for_bulk_import(self):
        """Configure PostgreSQL for maximum bulk import performance."""
        print("‚ö° Optimizing PostgreSQL for bulk import...")
        
        with self.conn.cursor() as cur:
            # Increase memory for bulk operations
            cur.execute("SET work_mem = '512MB'")
            cur.execute("SET maintenance_work_mem = '2GB'")
            
            # Disable synchronous commit for speed (data will still be durable)
            cur.execute("SET synchronous_commit = off")
            
            # Increase WAL buffers
            cur.execute("SET wal_buffers = '64MB'")
            
            # Disable autovacuum during import
            cur.execute("ALTER TABLE parcels SET (autovacuum_enabled = false)")
            cur.execute("ALTER TABLE cities SET (autovacuum_enabled = false)")
            cur.execute("ALTER TABLE counties SET (autovacuum_enabled = false)")
            
        self.conn.commit()
        print("‚úÖ PostgreSQL optimized for bulk import")
    
    def _restore_postgres_settings(self):
        """Restore normal PostgreSQL settings after import."""
        print("üîß Restoring normal PostgreSQL settings...")
        
        with self.conn.cursor() as cur:
            # Re-enable autovacuum
            cur.execute("ALTER TABLE parcels SET (autovacuum_enabled = true)")
            cur.execute("ALTER TABLE cities SET (autovacuum_enabled = true)")
            cur.execute("ALTER TABLE counties SET (autovacuum_enabled = true)")
            
            # Reset to defaults
            cur.execute("RESET work_mem")
            cur.execute("RESET maintenance_work_mem")
            cur.execute("RESET synchronous_commit")
            cur.execute("RESET wal_buffers")
            
            # Analyze tables for query planner
            cur.execute("ANALYZE parcels")
            cur.execute("ANALYZE cities")
            cur.execute("ANALYZE counties")
            
        self.conn.commit()
        print("‚úÖ PostgreSQL settings restored")
    
    def ensure_texas_state(self) -> str:
        """Ensure Texas state exists and return its ID."""
        if self.texas_state_id:
            return self.texas_state_id
            
        try:
            with self.conn.cursor() as cur:
                # Try to find existing Texas state
                cur.execute("SELECT id FROM states WHERE code = 'TX'")
                result = cur.fetchone()
                
                if result:
                    self.texas_state_id = result[0]
                    print("‚úÖ Found existing Texas state")
                else:
                    # Create Texas state
                    cur.execute("""
                        INSERT INTO states (code, name) 
                        VALUES ('TX', 'Texas') 
                        RETURNING id
                    """)
                    self.texas_state_id = cur.fetchone()[0]
                    self.conn.commit()
                    print("‚úÖ Created Texas state record")
            
            return self.texas_state_id
            
        except Exception as e:
            print(f"‚ùå Failed to ensure Texas state: {e}")
            raise
    
    def ensure_county(self, county_name: str, state_id: str) -> str:
        """Ensure county exists and return its ID."""
        if self.county_id:
            return self.county_id
            
        try:
            with self.conn.cursor() as cur:
                # Try to find existing county
                cur.execute("""
                    SELECT id FROM counties 
                    WHERE name = %s AND state_id = %s
                """, (county_name, state_id))
                result = cur.fetchone()
                
                if result:
                    self.county_id = result[0]
                    print(f"‚úÖ Found existing {county_name} County")
                else:
                    # Create new county
                    cur.execute("""
                        INSERT INTO counties (name, state_id) 
                        VALUES (%s, %s) 
                        RETURNING id
                    """, (county_name, state_id))
                    self.county_id = cur.fetchone()[0]
                    self.conn.commit()
                    self.stats['counties_created'] += 1
                    print(f"‚úÖ Created {county_name} County")
            
            return self.county_id
            
        except Exception as e:
            print(f"‚ùå Failed to ensure county {county_name}: {e}")
            raise
    
    def bulk_create_cities(self, city_names: set, county_id: str, state_id: str) -> Dict[str, str]:
        """Bulk create all unique cities at once using COPY FROM."""
        print(f"üèòÔ∏è  Bulk creating {len(city_names)} unique cities...")
        
        try:
            with self.conn.cursor() as cur:
                # First, get existing cities
                cur.execute("""
                    SELECT name, id FROM cities 
                    WHERE county_id = %s
                """, (county_id,))
                
                existing_cities = {name: city_id for name, city_id in cur.fetchall()}
                
                # Find cities that need to be created
                new_cities = city_names - set(existing_cities.keys())
                
                if new_cities:
                    # Prepare CSV data for COPY FROM
                    csv_buffer = StringIO()
                    writer = csv.writer(csv_buffer)
                    
                    for city_name in new_cities:
                        writer.writerow([city_name, county_id, state_id])
                    
                    csv_buffer.seek(0)
                    
                    # Use COPY FROM for bulk insert
                    cur.copy_expert("""
                        COPY cities (name, county_id, state_id) 
                        FROM STDIN WITH CSV
                    """, csv_buffer)
                    
                    # Get the newly created city IDs
                    cur.execute("""
                        SELECT name, id FROM cities 
                        WHERE county_id = %s AND name = ANY(%s)
                    """, (county_id, list(new_cities)))
                    
                    new_city_results = {name: city_id for name, city_id in cur.fetchall()}
                    existing_cities.update(new_city_results)
                    
                    self.stats['cities_created'] += len(new_cities)
                    print(f"  ‚úÖ Created {len(new_cities)} new cities")
                
                self.conn.commit()
                return existing_cities
                
        except Exception as e:
            print(f"‚ùå Failed to bulk create cities: {e}")
            self.conn.rollback()
            raise
    
    def bulk_import_parcels(self, df: pd.DataFrame, county_id: str, state_id: str, cities_map: Dict[str, str]):
        """Import parcels using ultra-fast PostgreSQL COPY FROM."""
        print(f"üì¶ Bulk importing {len(df):,} parcels using COPY FROM...")
        
        try:
            # Prepare CSV data in memory
            csv_buffer = StringIO()
            writer = csv.writer(csv_buffer)
            
            successful_rows = 0
            
            for idx, row in df.iterrows():
                try:
                    # Extract parcel data with flexible column matching
                    parcel_number = self._get_column_value(row, ['parcelnumb', 'parcel_number', 'parcel_id', 'account_number', 'account'])
                    address = self._get_column_value(row, ['address', 'saddress', 'property_address', 'site_address', 'location'])
                    owner_name = self._get_column_value(row, ['owner', 'owner_name', 'taxpayer_name', 'unmodified_owner'])
                    
                    # Get city ID
                    city_name = None
                    for col in ['city', 'scity', 'municipality']:
                        if col in row.index and pd.notna(row[col]) and str(row[col]).strip():
                            city_name = str(row[col]).strip().title()
                            break
                    
                    city_id = cities_map.get(city_name) if city_name else None
                    
                    # Numeric fields
                    property_value = self._parse_numeric(row, ['parval', 'property_value', 'market_value', 'appraised_value', 'total_value'])
                    lot_size = self._parse_numeric(row, ['gisacre', 'lot_size', 'acreage', 'acres', 'deeded_acres'])
                    
                    # Validate required fields
                    if not parcel_number or not address:
                        continue
                    
                    # Write to CSV buffer
                    writer.writerow([
                        str(parcel_number)[:100],  # parcel_number
                        str(address),              # address
                        city_id,                   # city_id
                        county_id,                 # county_id
                        state_id,                  # state_id
                        str(owner_name)[:255] if owner_name else None,  # owner_name
                        property_value,            # property_value
                        lot_size,                  # lot_size
                        None,                      # zoned_by_right
                        None,                      # occupancy_class
                        None                       # fire_sprinklers
                    ])
                    
                    successful_rows += 1
                    
                except Exception as e:
                    continue  # Skip problematic rows
            
            print(f"  üìù Prepared {successful_rows:,} rows for COPY FROM")
            
            # Perform bulk insert using COPY FROM
            csv_buffer.seek(0)
            
            with self.conn.cursor() as cur:
                start_time = time.time()
                
                cur.copy_expert("""
                    COPY parcels (parcel_number, address, city_id, county_id, state_id, 
                                 owner_name, property_value, lot_size, zoned_by_right, 
                                 occupancy_class, fire_sprinklers) 
                    FROM STDIN WITH CSV NULL ''
                """, csv_buffer)
                
                copy_time = time.time() - start_time
                rate = successful_rows / copy_time if copy_time > 0 else 0
                
                print(f"  ‚ö° COPY FROM completed: {successful_rows:,} records in {copy_time:.1f}s ({rate:.0f} records/sec)")
            
            self.conn.commit()
            self.stats['parcels_created'] += successful_rows
            self.stats['records_processed'] += len(df)
            
        except Exception as e:
            print(f"‚ùå Bulk import failed: {e}")
            self.conn.rollback()
            raise
    
    def _get_column_value(self, row: pd.Series, possible_columns: List[str]) -> Optional[str]:
        """Get value from row using list of possible column names."""
        for col in possible_columns:
            if col in row.index and pd.notna(row[col]) and str(row[col]).strip():
                return str(row[col]).strip()
        return None
    
    def _parse_numeric(self, row: pd.Series, possible_columns: List[str]) -> Optional[float]:
        """Parse numeric value from row using list of possible column names."""
        for col in possible_columns:
            if col in row.index and pd.notna(row[col]):
                try:
                    value_str = str(row[col]).replace('$', '').replace(',', '').strip()
                    if value_str and value_str != '' and value_str.lower() not in ['nan', 'null', 'none']:
                        return float(value_str)
                except (ValueError, TypeError):
                    continue
        return None
    
    def run_import(self):
        """Main import process using optimized PostgreSQL COPY FROM."""
        print("üöÄ SEEK Optimized Bulk Import - PostgreSQL COPY FROM")
        print("=" * 70)
        print(f"üìÅ File: {self.csv_file_path}")
        print(f"üìä Batch Size: {self.batch_size:,} records")
        print("=" * 70)
        
        if not self.csv_file_path.exists():
            print(f"‚ùå File not found: {self.csv_file_path}")
            sys.exit(1)
        
        try:
            # Optimize PostgreSQL settings
            self._optimize_postgres_for_bulk_import()
            
            # Load CSV file
            print("üìÑ Loading CSV file...")
            df = pd.read_csv(self.csv_file_path, dtype=str, keep_default_na=False)
            self.stats['total_records'] = len(df)
            print(f"‚úÖ Loaded {len(df):,} records")
            
            if df.empty:
                print("‚ö†Ô∏è  File is empty")
                return
            
            # Extract county name from filename
            county_name = self.csv_file_path.stem.replace('tx_', '').replace('_filtered_clean', '').replace('_', ' ').title()
            print(f"üèõÔ∏è  County: {county_name}")
            
            # Setup database entities
            print("\nüîß Setting up database entities...")
            state_id = self.ensure_texas_state()
            county_id = self.ensure_county(county_name, state_id)
            
            # Extract all unique city names for bulk creation
            print("üèòÔ∏è  Analyzing cities...")
            unique_cities = set()
            for col in ['city', 'scity', 'municipality']:
                if col in df.columns:
                    cities_in_col = df[col].dropna().astype(str).str.strip().str.title()
                    cities_in_col = cities_in_col[cities_in_col != '']
                    unique_cities.update(cities_in_col)
            
            print(f"  üìä Found {len(unique_cities)} unique cities")
            
            # Bulk create all cities at once
            cities_map = self.bulk_create_cities(unique_cities, county_id, state_id)
            
            # Import parcels in batches using COPY FROM
            print(f"\nüì¶ Processing {len(df):,} records...")
            
            start_time = time.time()
            
            for i in range(0, len(df), self.batch_size):
                batch_df = df.iloc[i:i + self.batch_size].copy()
                batch_num = (i // self.batch_size) + 1
                total_batches = (len(df) + self.batch_size - 1) // self.batch_size
                
                print(f"\nüì¶ Processing batch {batch_num}/{total_batches} ({len(batch_df):,} records)...")
                
                self.bulk_import_parcels(batch_df, county_id, state_id, cities_map)
                self.stats['batches_processed'] += 1
                
                # Progress update
                elapsed = time.time() - start_time
                processed = min(i + self.batch_size, len(df))
                rate = processed / elapsed if elapsed > 0 else 0
                remaining = len(df) - processed
                eta_seconds = remaining / rate if rate > 0 else 0
                
                print(f"  üìä Progress: {processed:,}/{len(df):,} ({processed/len(df)*100:.1f}%) | "
                      f"Rate: {rate:.0f}/sec | ETA: {eta_seconds/60:.1f}min")
            
            # Restore PostgreSQL settings
            self._restore_postgres_settings()
            
            # Final statistics
            self._print_final_stats()
            
        except Exception as e:
            print(f"‚ùå Import failed: {e}")
            self.conn.rollback()
            raise
        finally:
            if hasattr(self, 'conn'):
                self.conn.close()
    
    def _print_final_stats(self):
        """Print final import statistics."""
        elapsed = time.time() - self.stats['start_time'].timestamp()
        avg_rate = self.stats['records_processed'] / elapsed if elapsed > 0 else 0
        
        print("\n" + "=" * 70)
        print("üéâ OPTIMIZED IMPORT COMPLETED!")
        print("=" * 70)
        print(f"üìÅ File: {self.csv_file_path.name}")
        print(f"üìä Records processed: {self.stats['records_processed']:,}/{self.stats['total_records']:,}")
        print(f"üèõÔ∏è  Counties created: {self.stats['counties_created']}")
        print(f"üèòÔ∏è  Cities created: {self.stats['cities_created']}")
        print(f"üè† Parcels created: {self.stats['parcels_created']:,}")
        print(f"üì¶ Batches processed: {self.stats['batches_processed']}")
        print(f"‚ö†Ô∏è  Errors: {self.stats['errors']}")
        print(f"‚è±Ô∏è  Total time: {elapsed:.1f} seconds ({elapsed/60:.1f} minutes)")
        print(f"üìà Average rate: {avg_rate:.0f} records/second")
        print("=" * 70)
        
        # Performance assessment
        if avg_rate >= 10000:
            print("üöÄ EXCELLENT: Ultra-fast performance achieved!")
        elif avg_rate >= 1000:
            print("‚úÖ VERY GOOD: Great performance improvement!")
        elif avg_rate >= 100:
            print("‚úÖ GOOD: Significant performance improvement!")
        else:
            print("‚ö†Ô∏è  Performance could be better - check network/database configuration")

def main():
    """Main entry point."""
    if len(sys.argv) != 2:
        print("Usage: python optimized_bulk_import.py <csv_file_path>")
        print("Example: python optimized_bulk_import.py data/CleanedCsv/tx_bexar_filtered_clean.csv")
        sys.exit(1)
    
    csv_file = sys.argv[1]
    
    try:
        importer = OptimizedBulkImporter(csv_file)
        importer.run_import()
    except KeyboardInterrupt:
        print("\n‚èπÔ∏è  Import cancelled by user")
        sys.exit(0)
    except Exception as e:
        print(f"‚ùå Import failed: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()